{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUhHUW5qpctQ"
   },
   "source": [
    "# COMP5329 - Deep Learning\n",
    "\n",
    "## Tutorial 4 - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2ZCOJhUpctS"
   },
   "source": [
    "**Semester 1, 2021**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To learn about regularization.\n",
    "* To be familiar with how the regularization methods work, i.e., L2 regularization, dropout, batch normalization, early stopping, etc.\n",
    "* To learn how to implement regularization methods with deep learning frameworks (in this tutorial we use pytorch).\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Install pytorch and related packages into your anaconda environment.\n",
    "* Read and run this \"4-reg.ipynb\" file.\n",
    "* Complete the exercises.\n",
    "\n",
    "Lecturers: Chang Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNXqhSSGpctU"
   },
   "source": [
    "## 0. Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "-zeFRtbrpctX",
    "outputId": "44cc47f2-2430-41d0-b6cf-f82f6d9f4198"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-faa311522481>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiCcM4_fpctj"
   },
   "source": [
    "## 1. Hyperparameters\n",
    "Defination of hyperparameters:\n",
    "\n",
    "N_SAMPLES : number of samples\n",
    "\n",
    "NOISE_RATE : amplitude of noise\n",
    "\n",
    "N_INPUT_LAYER : dimension of input\n",
    "\n",
    "N_HIDDEN_LAYER : dimension of hidden layer\n",
    "\n",
    "N_OUTPUT_LAYER : dimension of output layer\n",
    "\n",
    "N_HIDDEN : number of hidden layer\n",
    "\n",
    "BATCH_SIZE : batch size for training\n",
    "\n",
    "EPOCH : training epoch\n",
    "\n",
    "LEARNING_RATE : learning rate\n",
    "\n",
    "DROPOUT_RATE : dropout rate\n",
    "\n",
    "WEIGHT_DECAY : value of penalty term of L2 regularization\n",
    "\n",
    "MAX_COUNT : parameter related to the early stopping criterion\n",
    "\n",
    "ACTIVATION : activation function\n",
    "\n",
    "LOSS_FUNC : loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pp49Ax3Lpctl"
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 20\n",
    "NOISE_RATE = 0.4\n",
    "N_INPUT_LAYER = 1\n",
    "N_HIDDEN_LAYER = 100\n",
    "N_OUTPUT_LAYER = 1\n",
    "N_HIDDEN = 1\n",
    "BATCH_SIZE = 10\n",
    "EPOCH = 500\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT_RATE = 0.5\n",
    "WEIGHT_DECAY = 5e-3\n",
    "MAX_COUNT = 5\n",
    "ACTIVATION = nn.ReLU()\n",
    "LOSS_FUNC = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3xoOvLXpcts"
   },
   "source": [
    "## 2. The Dataset\n",
    "In this tutorial, we use the MLP to fit a linear function with noise under normal distribution, which is formulated as following:\n",
    "$$y=x+Norm(0,NOISE\\_RATE).$$\n",
    "For the training and testing sets, we evenly sample 20 points in the domain of $x\\in[-1, 1]$; while the validation set is half the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QomJPfFcpctu"
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "train_x = np.linspace(-1, 1, num=int(N_SAMPLES))[:, np.newaxis]\n",
    "noise = np.random.normal(0, NOISE_RATE, train_x.shape)\n",
    "train_y = train_x + noise\n",
    "\n",
    "# test data\n",
    "test_x = np.linspace(-1, 1, num=int(N_SAMPLES))[:, np.newaxis]\n",
    "noise = np.random.normal(0, NOISE_RATE, test_x.shape)\n",
    "test_y = test_x + noise\n",
    "\n",
    "# validation data\n",
    "validate_x = np.linspace(-1, 1, num=int(N_SAMPLES/2))[:, np.newaxis]\n",
    "noise = np.random.normal(0, NOISE_RATE, validate_x.shape)\n",
    "validate_y = validate_x + noise\n",
    "\n",
    "train_x, train_y = torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float()\n",
    "test_x, test_y = torch.from_numpy(test_x).float(), torch.from_numpy(test_y).float()\n",
    "validate_x, validate_y = torch.from_numpy(validate_x).float(), torch.from_numpy(validate_y).float()\n",
    "\n",
    "train_dataset = Data.TensorDataset(train_x, train_y)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao8vKaIpctz"
   },
   "source": [
    "## 3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIrf23g-pct1"
   },
   "source": [
    "### 3.1 Vanilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NM-VTa3gpct1"
   },
   "outputs": [],
   "source": [
    "class FC_Classifier(nn.Module):\n",
    "    \"\"\"Custom module for a simple MLP\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FC_Classifier, self).__init__()\n",
    "        self.fcs = []\n",
    "        self.fc_i = nn.Linear(N_INPUT_LAYER, N_HIDDEN_LAYER)\n",
    "\n",
    "        # define and name every hidden layer in the module\n",
    "        for i in range(N_HIDDEN):\n",
    "            fc = nn.Linear(N_HIDDEN_LAYER, N_HIDDEN_LAYER)\n",
    "            setattr(self, 'fc%i' % i, fc)  # IMPORTANT set layer to the Module, if not doing so, the fc will not belong to the module\n",
    "            self._set_init(fc)  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "\n",
    "        self.fc_o = nn.Linear(N_HIDDEN_LAYER, N_OUTPUT_LAYER)\n",
    "        self._set_init(self.fc_i)\n",
    "        self._set_init(self.fc_o)\n",
    "\n",
    "    def _set_init(self, layer):\n",
    "        init.normal_(layer.weight, mean=0., std=.1)\n",
    "        init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, N_INPUT_LAYER)\n",
    "        x = ACTIVATION(self.fc_i(x))\n",
    "        \n",
    "        for i in range(N_HIDDEN):\n",
    "            x = ACTIVATION(self.fcs[i](x))\n",
    "            \n",
    "        x = self.fc_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgL-8zLOpct7"
   },
   "source": [
    "### 3.2 MLP with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2zG7_cj9pct8"
   },
   "outputs": [],
   "source": [
    "class Dropout_Classifier(nn.Module):\n",
    "    \"\"\"Custom module for a MLP with dropout\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dropout_Classifier, self).__init__()\n",
    "        self.fcs = []\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.fc_i = nn.Linear(N_INPUT_LAYER, N_HIDDEN_LAYER)\n",
    "\n",
    "        # define and name every hidden layer in the module\n",
    "        for i in range(N_HIDDEN):\n",
    "            fc = nn.Linear(N_HIDDEN_LAYER, N_HIDDEN_LAYER)\n",
    "            setattr(self, 'fc%i' % i, fc)  # IMPORTANT set layer to the Module, if not doing so, the fc will not belong to the module\n",
    "            self._set_init(fc)  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "\n",
    "        self.fc_o = nn.Linear(N_HIDDEN_LAYER, N_OUTPUT_LAYER)\n",
    "        self._set_init(self.fc_i)\n",
    "        self._set_init(self.fc_o)\n",
    "\n",
    "\n",
    "    def _set_init(self, layer):\n",
    "        init.normal_(layer.weight, mean=0., std=.1)\n",
    "        init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, N_INPUT_LAYER)\n",
    "        x = self.fc_i(x)\n",
    "        x = self.dropout(x)\n",
    "        x = ACTIVATION(x)\n",
    "\n",
    "        for i in range(N_HIDDEN):\n",
    "            x = self.fcs[i](x)\n",
    "            # IMPORTANT: when implement dropout with F.dropout(), please use training=self.training\n",
    "            x = F.dropout(x, p=DROPOUT_RATE, training=self.training) \n",
    "            x = ACTIVATION(x)\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdH8XSaopcuA"
   },
   "source": [
    "### 3.3 MLP with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "x_4Q-ZT1pcuB"
   },
   "outputs": [],
   "source": [
    "class Batch_Normalization_Classifier(nn.Module):\n",
    "    \"\"\"Custom module for a MLP with batch normalization\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Batch_Normalization_Classifier, self).__init__()\n",
    "        self.fcs = []\n",
    "        self.bns = []\n",
    "        self.fc_i = nn.Linear(N_INPUT_LAYER, N_HIDDEN_LAYER)\n",
    "        self.input_bn = nn.BatchNorm1d(N_INPUT_LAYER)\n",
    "        self.first_bn = nn.BatchNorm1d(N_HIDDEN_LAYER)\n",
    "\n",
    "        # define and name every hidden layer in the module\n",
    "        for i in range(N_HIDDEN):\n",
    "            fc = nn.Linear(N_HIDDEN_LAYER, N_HIDDEN_LAYER)\n",
    "            setattr(self, 'fc%i' % i, fc)  # IMPORTANT set layer to the Module, if not doing so, the fc will not belong to the module\n",
    "            self._set_init(fc)  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "            bn = nn.BatchNorm1d(N_HIDDEN_LAYER)\n",
    "            setattr(self, 'bn%i' % i, bn)  # IMPORTANT set layer to the Module, if not doing so, the fc will not belong to the module\n",
    "            self.bns.append(bn)\n",
    "\n",
    "        self.fc_o = nn.Linear(N_HIDDEN_LAYER, N_OUTPUT_LAYER)\n",
    "        self._set_init(self.fc_i)\n",
    "        self._set_init(self.fc_o)\n",
    "\n",
    "    def _set_init(self, layer):\n",
    "        init.normal_(layer.weight, mean=0., std=.1)\n",
    "        init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, N_INPUT_LAYER)\n",
    "        x = self.input_bn(x)\n",
    "        x = self.fc_i(x)\n",
    "        x = ACTIVATION(self.first_bn(x))\n",
    "        \n",
    "        for i in range(N_HIDDEN):\n",
    "            x = self.fcs[i](x)\n",
    "            x = self.bns[i](x)  # batch normalization\n",
    "            x = ACTIVATION(x)\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li-vtSL1pcuF"
   },
   "source": [
    "## 4. Build Networks and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4FA-C-SVpcuF"
   },
   "outputs": [],
   "source": [
    "fc_net = FC_Classifier()\n",
    "l2_net = FC_Classifier()\n",
    "early_stop_net = FC_Classifier()\n",
    "dropped_net = Dropout_Classifier()\n",
    "bned_net = Batch_Normalization_Classifier()\n",
    "\n",
    "fc_opt = torch.optim.Adam(fc_net.parameters(), lr=LEARNING_RATE)\n",
    "l2_opt = torch.optim.Adam(l2_net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "early_stop_opt = torch.optim.Adam(early_stop_net.parameters(), lr=LEARNING_RATE)\n",
    "dropped_opt = torch.optim.Adam(dropped_net.parameters(), lr=LEARNING_RATE)\n",
    "bned_opt = optim.Adam(bned_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "nets = [fc_net, l2_net, dropped_net, bned_net]\n",
    "opts = [fc_opt, l2_opt, dropped_opt, bned_opt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl3yQU6UpcuK"
   },
   "source": [
    "## 5. Training and Testing\n",
    "* In this tutorial, the early stopping criterion is that if the validation loss does not decrease after 50 epochs, we stop training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "galVkmzvpcuK"
   },
   "outputs": [],
   "source": [
    "is_early_stop = False\n",
    "last_validation_loss = LOSS_FUNC(early_stop_net(validate_x), validate_y).data.numpy()\n",
    "flag = 'training'\n",
    "count = 0\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # dataset API gives us pythonic batching\n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        target = Variable(label)\n",
    "        for net, opt in zip(nets, opts):  # train for each network\n",
    "            preds = net(data)\n",
    "            loss = LOSS_FUNC(preds, target)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()  # it will also learns the parameters in Batch Normalization\n",
    "            # loss_history[nets.index(net)].append(loss.data)\n",
    "        if not is_early_stop:\n",
    "            preds = early_stop_net(data)\n",
    "            loss = LOSS_FUNC(preds, target)\n",
    "            early_stop_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            early_stop_opt.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # change to eval mode in order to fix drop out effect\n",
    "        for net in nets:\n",
    "            net.eval()# parameters for dropout differ from train mode\n",
    "        early_stop_net.eval()\n",
    "\n",
    "        if not is_early_stop:\n",
    "            validate_pred_early_stop = early_stop_net(validate_x)\n",
    "\n",
    "            if LOSS_FUNC(validate_pred_early_stop, validate_y).data.numpy() > last_validation_loss:\n",
    "                count += 1\n",
    "            else:\n",
    "                last_validation_loss = LOSS_FUNC(validate_pred_early_stop, validate_y).data.numpy()\n",
    "                count = 0\n",
    "\n",
    "            if count == MAX_COUNT:\n",
    "                print('early stopped!!!')\n",
    "                flag = 'stopped'\n",
    "                is_early_stop = True\n",
    "\n",
    "        # plotting\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(15,10))\n",
    "        test_pred_fc = fc_net(test_x)\n",
    "        test_pred_l2 = l2_net(test_x)\n",
    "        test_pred_early_stop = early_stop_net(test_x)\n",
    "        test_pred_drop = dropped_net(test_x)\n",
    "        test_pred_bn = bned_net(test_x)\n",
    "        plt.scatter(train_x.data.numpy(), train_y.data.numpy(), c='magenta', s=50, alpha=0.3, label='train')\n",
    "        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.3, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_fc.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_l2.data.numpy(), 'y-v', lw=3, label='L2 regularization')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_early_stop.data.numpy(), 'k-*', lw=3, label='early stopping')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout({})'.format(DROPOUT_RATE))\n",
    "        plt.plot(test_x.data.numpy(), test_pred_bn.data.numpy(), 'g-.', lw=3, label='batch normalization')\n",
    "        plt.text(0.1, -1.2, 'overfitting loss=%.4f' % LOSS_FUNC(test_pred_fc, test_y).data.numpy(), fontdict={'size': 20, 'color':  'r'})\n",
    "        plt.text(0.1, -1.5, 'L2 regularization loss=%.4f' % LOSS_FUNC(test_pred_l2, test_y).data.numpy(), fontdict={'size': 20, 'color':  'y'})\n",
    "        plt.text(0.1, -1.8, 'early stopping loss=%.4f' % LOSS_FUNC(test_pred_early_stop, test_y).data.numpy() + flag, fontdict={'size': 20, 'color': 'k'})\n",
    "        plt.text(0.1, -2.1, 'dropout loss=%.4f' % LOSS_FUNC(test_pred_drop, test_y).data.numpy(), fontdict={'size': 20, 'color': 'b'})\n",
    "        plt.text(0.1, -2.4, 'batch normalization loss=%.4f' % LOSS_FUNC(test_pred_bn, test_y).data.numpy(), fontdict={'size': 20, 'color': 'g'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));plt.pause(0.1)\n",
    "        plt.show()\n",
    "        # change back to train mode\n",
    "        for net in nets:\n",
    "            net.train()\n",
    "        early_stop_net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rfugFHzpcuQ"
   },
   "source": [
    "## 6. Exercises\n",
    "\n",
    "* Try different configurations of networks (e.g., structure, hyperparameters), and different traning sets.\n",
    "* Set your own early stopping criterion."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4-reg.ipynb",
   "provenance": [
    {
     "file_id": "1KmANXRSk8g21tVtoDNNJSyoCU_kmnFlZ",
     "timestamp": 1616455708375
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
